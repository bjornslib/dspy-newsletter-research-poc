# DSPy Newsletter Research Tool - Solution Design Document
# Pattern Recognition Architecture Approach

**Document Version**: 1.0
**Created**: 2026-01-12
**Architect**: Architect 2 (Pattern Recognition Strategy)
**Status**: Design Complete - Ready for Implementation

---

## EXECUTIVE SUMMARY

This solution design transforms PreEmploymentDirectory's manual newsletter research process into an intelligent DSPy-powered automation pipeline. By applying proven patterns from news aggregation systems (Google News, Flipboard), content classification platforms, and modern RAG architectures, we deliver a cost-effective PoC that achieves 70%+ recall while reducing 20+ hours/week of manual work.

**Core Innovation**: Multi-stage event-driven pipeline with hybrid keyword-semantic scoring, leveraging DSPy's optimization framework to automatically tune prompts and classification logic from labeled examples.

**Business Impact**:
- Transform 500+ daily articles into 20-50 high-quality candidates
- Enable natural language queries over historical newsletter corpus
- Establish foundation for scaling to real-time monitoring and personalized feeds

---

## 1. SYSTEM ARCHITECTURE

### 1.1 High-Level Architecture (ASCII Diagram)

```
┌─────────────────────────────────────────────────────────────────────────┐
│                      INGESTION LAYER (Batch - Daily)                     │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                           │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐                │
│  │ RSS      │  │ Web      │  │ Email    │  │ Social   │                │
│  │ Fetcher  │  │ Scraper  │  │ Parser   │  │ Monitor  │                │
│  └────┬─────┘  └────┬─────┘  └────┬─────┘  └────┬─────┘                │
│       │             │             │             │                        │
│       └─────────────┴─────────────┴─────────────┘                        │
│                          │                                               │
│                          ▼                                               │
│              ┌───────────────────────┐                                   │
│              │ Article Normalizer    │  ← Extract: title, content,      │
│              │ (HTML→Text, Metadata) │    date, source, URL             │
│              └───────────┬───────────┘                                   │
│                          │                                               │
│                   [Raw Articles Queue]                                   │
└──────────────────────────┬──────────────────────────────────────────────┘
                           │
                           ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                   DEDUPLICATION LAYER (Pre-Classification)               │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                           │
│  ┌────────────────────────────────────────────────────────────────┐     │
│  │ URL Normalization + Content Hash (Exact Dedup)                 │     │
│  │ - Remove tracking params, normalize URLs                        │     │
│  │ - SHA-256 hash of cleaned text                                  │     │
│  └──────────────────────┬─────────────────────────────────────────┘     │
│                         │                                                │
│                         ▼                                                │
│  ┌────────────────────────────────────────────────────────────────┐     │
│  │ SimHash Near-Duplicate Detection                                │     │
│  │ - 64-bit SimHash on 3-word shingles                             │     │
│  │ - Hamming distance threshold = 3 (allows minor edits)           │     │
│  │ - LSH buckets for fast candidate retrieval                      │     │
│  └──────────────────────┬─────────────────────────────────────────┘     │
│                         │                                                │
│                [Unique Articles Queue]                                   │
└─────────────────────────┬────────────────────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                    DSPy CLASSIFICATION PIPELINE                          │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                           │
│  Stage 1: RELEVANCE FILTERING                                            │
│  ┌────────────────────────────────────────────────────────────────┐     │
│  │ DSPy Module: RelevanceClassifier (ChainOfThought)              │     │
│  │ Input: title, content_preview (first 500 chars), keywords      │     │
│  │ Output: is_relevant (bool), confidence (0-1), reasoning        │     │
│  │ Optimization: BootstrapFewShot with 50 labeled examples        │     │
│  └──────────────────────┬─────────────────────────────────────────┘     │
│                         │                                                │
│                    [Relevant Articles]                                   │
│                         │                                                │
│  Stage 2: REGION CLASSIFICATION                                          │
│  ┌────────────────────────────────────────────────────────────────┐     │
│  │ DSPy Module: RegionClassifier (TypedPredictor)                 │     │
│  │ Input: title, content, extracted_entities                       │     │
│  │ Output: region (Literal[6 regions]), country (str|None)        │     │
│  │ Pattern: Geo-entity extraction → region mapping logic          │     │
│  └──────────────────────┬─────────────────────────────────────────┘     │
│                         │                                                │
│  Stage 3: TOPIC MULTI-LABEL CLASSIFICATION                               │
│  ┌────────────────────────────────────────────────────────────────┐     │
│  │ DSPy Module: TopicClassifier (TypedPredictor)                  │     │
│  │ Input: title, content, keywords_matched, entities               │     │
│  │ Output: topics (List[Literal[8 topics]]), primary_topic        │     │
│  │ Optimization: MIPROv2 for multi-label accuracy                 │     │
│  └──────────────────────┬─────────────────────────────────────────┘     │
│                         │                                                │
│  Stage 4: HYBRID RELEVANCE SCORING                                       │
│  ┌────────────────────────────────────────────────────────────────┐     │
│  │ Hybrid Scorer (Keyword + Semantic)                              │     │
│  │ - Keyword score: TF-IDF weighted match on 50+ keywords         │     │
│  │ - Semantic score: Cosine similarity to topic embeddings        │     │
│  │ - Combined: 0.4*keyword + 0.6*semantic                          │     │
│  │ - Freshness boost: +0.1 for articles <7 days old               │     │
│  └──────────────────────┬─────────────────────────────────────────┘     │
│                         │                                                │
│                [Classified Articles]                                     │
└─────────────────────────┬────────────────────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                   EMBEDDING & STORAGE LAYER                              │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                           │
│  ┌────────────────────────────────────────────────────────────────┐     │
│  │ Embedding Generation (Async)                                    │     │
│  │ - Model: text-embedding-3-small (OpenAI)                        │     │
│  │ - Granularity: Title+Summary (512 tokens max)                   │     │
│  │ - Dimension: 1536                                                │     │
│  └──────────────────────┬─────────────────────────────────────────┘     │
│                         │                                                │
│                         ▼                                                │
│  ┌──────────────────┬──────────────────────────────────────────┐        │
│  │                  │                                           │        │
│  │  ChromaDB        │      JSON Document Store                 │        │
│  │  (Vector Index)  │      (Primary Storage)                   │        │
│  │                  │                                           │        │
│  │  - Collections   │  - articles/                             │        │
│  │    by region     │    YYYY-MM-DD.json                       │        │
│  │  - Metadata:     │  - Full article metadata                 │        │
│  │    region,       │  - Classification results                │        │
│  │    topics,       │  - Dedup clusters                        │        │
│  │    date, score   │  - Embeddings reference                  │        │
│  │  - HNSW index    │                                           │        │
│  │    (M=16, ef=200)│                                           │        │
│  └──────────────────┴──────────────────────────────────────────┘        │
│                                                                           │
└─────────────────────────┬────────────────────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                      QUERY & RETRIEVAL LAYER (CLI)                       │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                           │
│  ┌────────────────────────────────────────────────────────────────┐     │
│  │ DSPy RAG Module: NewsletterQueryAgent                          │     │
│  │                                                                  │     │
│  │ Components:                                                      │     │
│  │ 1. QueryDecomposer (ChainOfThought)                             │     │
│  │    - Natural language → structured filters                      │     │
│  │    - Extract: regions, topics, date_range, keywords             │     │
│  │                                                                  │     │
│  │ 2. HybridRetriever (Custom Module)                              │     │
│  │    - Keyword search: BM25 over titles/content                   │     │
│  │    - Vector search: ChromaDB.query(query_embedding, k=20)       │     │
│  │    - Merge: RRF (Reciprocal Rank Fusion)                        │     │
│  │                                                                  │     │
│  │ 3. AnswerGenerator (ChainOfThought)                             │     │
│  │    - Synthesize response from retrieved articles                │     │
│  │    - Include: citations, summary, trend analysis                │     │
│  └────────────────────────────────────────────────────────────────┘     │
│                                                                           │
│  Query Examples:                                                         │
│  - "What are recent GDPR changes in Europe?"                             │
│  - "Show me Ban the Box updates in North America last 30 days"          │
│  - "Criminal background check regulations in APAC this quarter"         │
│                                                                           │
└───────────────────────────────────────────────────────────────────────────┘
```

### 1.2 Data Flow & Processing Model

**Processing Cadence**: Daily batch (runs at 2 AM UTC)

1. **Ingestion Phase** (30 min)
   - Fetch 500+ articles from 247 sources
   - Parse HTML, extract metadata
   - Output: ~500 raw articles/day

2. **Deduplication Phase** (10 min)
   - Exact dedup: ~15% reduction (75 duplicates removed)
   - Near-dedup (SimHash): ~10% reduction (50 near-duplicates)
   - Output: ~375 unique articles

3. **Classification Phase** (45 min)
   - Relevance filtering: ~60% pass rate → 225 relevant articles
   - Region + Topic classification: all 225 articles
   - Hybrid scoring: all 225 articles
   - Output: 225 classified, scored articles

4. **Embedding & Storage Phase** (20 min)
   - Generate embeddings: 225 articles
   - Store in ChromaDB + JSON
   - Update search indices

5. **Daily Shortlist Generation** (5 min)
   - Rank by relevance score
   - Apply diversity filters (max 3 per source)
   - Output: Top 20-50 articles for human review

**Total Pipeline Duration**: ~2 hours
**Expected Throughput**: 225 processed articles/day
**Storage Growth**: ~50MB/day (JSON + embeddings)

---

## 2. DSPy SIGNATURES & MODULES

### 2.1 Relevance Classification Signature

```python
from typing import Literal
import dspy

class RelevanceSignature(dspy.Signature):
    """
    Determine if an article is relevant to background screening industry.

    Relevant articles discuss: employment screening, background checks,
    criminal records, credential verification, right-to-work, Ban the Box,
    FCRA/GDPR compliance, drug testing, reference checks, or related regulations.
    """

    title: str = dspy.InputField(
        desc="Article title"
    )

    content_preview: str = dspy.InputField(
        desc="First 500 characters of article content"
    )

    keywords_matched: list[str] = dspy.InputField(
        desc="List of primary/secondary keywords found in title or content"
    )

    is_relevant: bool = dspy.OutputField(
        desc="True if article is relevant to background screening industry"
    )

    confidence: float = dspy.OutputField(
        desc="Confidence score between 0.0 and 1.0"
    )

    reasoning: str = dspy.OutputField(
        desc="Brief explanation (1-2 sentences) of classification decision"
    )


class RelevanceClassifier(dspy.Module):
    """
    ChainOfThought-based relevance classifier.
    Uses reasoning to improve edge case handling (e.g., HR tech not about screening).
    """

    def __init__(self):
        super().__init__()
        self.classifier = dspy.ChainOfThought(RelevanceSignature)

    def forward(self, title: str, content_preview: str, keywords_matched: list[str]):
        prediction = self.classifier(
            title=title,
            content_preview=content_preview,
            keywords_matched=keywords_matched
        )
        return prediction
```

**Design Rationale**:
- **ChainOfThought**: Handles nuanced cases (e.g., "employee screening" in healthcare vs employment context)
- **Keywords as input**: Hybrid approach - keyword pre-filter guides LLM reasoning
- **Reasoning output**: Enables error analysis and debugging of false positives/negatives

### 2.2 Region Classification Signature

```python
from typing import Literal, Optional

RegionEnum = Literal[
    "africa_middle_east",
    "asia_pacific",
    "europe",
    "north_america_caribbean",
    "south_america",
    "worldwide"
]

class RegionSignature(dspy.Signature):
    """
    Classify article by primary geographic region.

    Use 'worldwide' for cross-regional topics (e.g., "GDPR impact on global screening").
    Extract country when mentioned explicitly.
    """

    title: str = dspy.InputField()

    content: str = dspy.InputField(
        desc="Full article content (max 2000 tokens)"
    )

    extracted_entities: list[str] = dspy.InputField(
        desc="List of geographic entities (countries, regions, cities) found in text"
    )

    region: RegionEnum = dspy.OutputField(
        desc="Primary geographic region covered by article"
    )

    country: Optional[str] = dspy.OutputField(
        desc="Specific country if clearly identified, else None"
    )

    confidence: float = dspy.OutputField(
        desc="Confidence score 0.0-1.0"
    )


class RegionClassifier(dspy.Module):
    """
    TypedPredictor for structured region classification.
    Fast inference with type safety.
    """

    def __init__(self):
        super().__init__()
        # TypedPredictor ensures type-safe outputs matching RegionEnum
        self.predict = dspy.TypedPredictor(RegionSignature)

    def forward(self, title: str, content: str, extracted_entities: list[str]):
        result = self.predict(
            title=title,
            content=content[:8000],  # Truncate to ~2000 tokens
            extracted_entities=extracted_entities
        )
        return result
```

**Design Rationale**:
- **TypedPredictor**: Enforces enum constraints (no hallucinated regions)
- **Entity extraction pre-processing**: Simple spaCy NER pipeline feeds geographic entities
- **Worldwide handling**: Avoids forcing artificial single-region assignment for global topics

### 2.3 Topic Multi-Label Classification Signature

```python
from typing import List

TopicEnum = Literal[
    "regulatory_legal",
    "criminal_background",
    "education_verification",
    "immigration_work_authorization",
    "industry_ma_news",
    "technology_products",
    "conferences_events",
    "court_cases_precedents"
]

class TopicSignature(dspy.Signature):
    """
    Multi-label topic classification for background screening articles.

    An article can belong to multiple topics (e.g., regulatory + criminal background).
    Select ALL applicable topics. Identify the single most relevant as primary_topic.
    """

    title: str = dspy.InputField()

    content: str = dspy.InputField(
        desc="Article content (max 2000 tokens)"
    )

    keywords_matched: list[str] = dspy.InputField(
        desc="Relevant keywords found in article"
    )

    extracted_entities: list[str] = dspy.InputField(
        desc="Named entities (orgs, laws, people) from article"
    )

    topics: List[TopicEnum] = dspy.OutputField(
        desc="List of ALL applicable topics (minimum 1, usually 1-3)"
    )

    primary_topic: TopicEnum = dspy.OutputField(
        desc="Single most relevant topic for this article"
    )

    confidence: float = dspy.OutputField(
        desc="Overall classification confidence 0.0-1.0"
    )


class TopicClassifier(dspy.Module):
    """
    Multi-label topic classifier with MIPROv2 optimization.
    """

    def __init__(self):
        super().__init__()
        self.predict = dspy.TypedPredictor(TopicSignature)

    def forward(self, title: str, content: str, keywords_matched: list[str],
                extracted_entities: list[str]):
        result = self.predict(
            title=title,
            content=content[:8000],
            keywords_matched=keywords_matched,
            extracted_entities=extracted_entities
        )

        # Validation: ensure primary_topic is in topics list
        if result.primary_topic not in result.topics:
            result.topics.append(result.primary_topic)

        return result
```

**Design Rationale**:
- **Multi-label output**: Reflects real-world articles covering multiple themes
- **Primary topic**: Enables single-topic filtering/grouping when needed
- **Rich input context**: Keywords + entities guide classification (hybrid approach)

### 2.4 Hybrid Relevance Scoring Module

```python
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from datetime import datetime, timedelta

class HybridRelevanceScorer:
    """
    Non-LLM hybrid scorer combining keyword (TF-IDF) and semantic (embedding) signals.

    This is NOT a DSPy module - it's a traditional ML component for cost efficiency.
    """

    def __init__(self, keyword_list: list[str], topic_embeddings: dict[str, np.ndarray]):
        """
        Args:
            keyword_list: 50+ primary/secondary keywords
            topic_embeddings: Pre-computed embeddings for each of 8 topics
        """
        self.keyword_list = keyword_list
        self.topic_embeddings = topic_embeddings

        # TF-IDF vectorizer for keyword scoring
        self.tfidf = TfidfVectorizer(
            vocabulary=keyword_list,
            lowercase=True,
            stop_words='english'
        )

    def score_article(
        self,
        title: str,
        content: str,
        article_embedding: np.ndarray,
        classified_topics: list[str],
        published_date: datetime
    ) -> dict:
        """
        Compute hybrid relevance score.

        Returns:
            {
                'keyword_score': float,
                'semantic_score': float,
                'freshness_boost': float,
                'final_score': float
            }
        """
        # 1. Keyword score (TF-IDF weighted match)
        combined_text = f"{title} {content[:2000]}"
        tfidf_matrix = self.tfidf.fit_transform([combined_text])
        keyword_score = float(tfidf_matrix.sum())  # Sum of TF-IDF weights

        # Normalize to 0-1 range (empirically set threshold)
        keyword_score = min(keyword_score / 10.0, 1.0)

        # 2. Semantic score (cosine similarity to topic embeddings)
        topic_similarities = []
        for topic in classified_topics:
            if topic in self.topic_embeddings:
                sim = cosine_similarity(
                    article_embedding.reshape(1, -1),
                    self.topic_embeddings[topic].reshape(1, -1)
                )[0][0]
                topic_similarities.append(sim)

        semantic_score = max(topic_similarities) if topic_similarities else 0.0

        # 3. Freshness boost
        days_old = (datetime.now() - published_date).days
        freshness_boost = 0.1 if days_old <= 7 else 0.0

        # 4. Combined score (weighted average + boost)
        final_score = (0.4 * keyword_score + 0.6 * semantic_score) + freshness_boost
        final_score = min(final_score, 1.0)  # Cap at 1.0

        return {
            'keyword_score': keyword_score,
            'semantic_score': semantic_score,
            'freshness_boost': freshness_boost,
            'final_score': final_score
        }
```

**Design Rationale**:
- **Non-LLM component**: Cost optimization - no API calls for 225 articles/day
- **Hybrid approach**: Keyword catches explicit mentions, semantic catches paraphrases
- **Freshness boost**: News recency matters for screening industry updates
- **Tunable weights**: 0.4/0.6 split based on pattern from news aggregation systems

### 2.5 RAG Query Pipeline Signatures

```python
class QueryDecompositionSignature(dspy.Signature):
    """
    Decompose natural language query into structured search filters.

    Example:
    Query: "Ban the Box updates in California last 30 days"
    Output: regions=['north_america_caribbean'], topics=['regulatory_legal'],
            keywords=['Ban the Box', 'California'], date_range={'days': 30}
    """

    query: str = dspy.InputField(
        desc="Natural language query from user"
    )

    regions: List[RegionEnum] = dspy.OutputField(
        desc="List of relevant regions (empty list = all regions)"
    )

    topics: List[TopicEnum] = dspy.OutputField(
        desc="List of relevant topics (empty list = all topics)"
    )

    keywords: List[str] = dspy.OutputField(
        desc="Key terms to search for in article titles/content"
    )

    date_range: dict = dspy.OutputField(
        desc="Date filter as {days: int} for 'last N days', or {} for no filter"
    )


class AnswerGenerationSignature(dspy.Signature):
    """
    Generate comprehensive answer to user query based on retrieved articles.
    """

    query: str = dspy.InputField(
        desc="Original user question"
    )

    retrieved_articles: str = dspy.InputField(
        desc="Top K retrieved articles with titles, summaries, sources, dates"
    )

    answer: str = dspy.OutputField(
        desc="Comprehensive answer synthesizing information from articles"
    )

    citations: List[str] = dspy.OutputField(
        desc="List of article titles/URLs cited in answer"
    )

    trend_summary: str = dspy.OutputField(
        desc="1-2 sentence summary of trends or patterns observed across articles"
    )


class NewsletterRAG(dspy.Module):
    """
    Complete RAG pipeline for newsletter queries.
    """

    def __init__(self, chroma_client, json_store_path: str):
        super().__init__()

        self.chroma_client = chroma_client
        self.json_store = json_store_path

        # DSPy components
        self.decompose_query = dspy.ChainOfThought(QueryDecompositionSignature)
        self.generate_answer = dspy.ChainOfThought(AnswerGenerationSignature)

    def forward(self, query: str, k: int = 20):
        # Step 1: Decompose query into structured filters
        decomposition = self.decompose_query(query=query)

        # Step 2: Hybrid retrieval (keyword + vector)
        retrieved_articles = self._hybrid_retrieve(
            keywords=decomposition.keywords,
            regions=decomposition.regions,
            topics=decomposition.topics,
            date_range=decomposition.date_range,
            k=k
        )

        # Step 3: Format articles for context
        context = self._format_articles(retrieved_articles)

        # Step 4: Generate answer
        result = self.generate_answer(
            query=query,
            retrieved_articles=context
        )

        return dspy.Prediction(
            answer=result.answer,
            citations=result.citations,
            trend_summary=result.trend_summary,
            retrieved_articles=retrieved_articles  # Pass through for inspection
        )

    def _hybrid_retrieve(self, keywords: list[str], regions: list[str],
                        topics: list[str], date_range: dict, k: int):
        """
        Hybrid retrieval using BM25 + ChromaDB vector search + RRF fusion.
        """
        # Build ChromaDB query
        query_text = " ".join(keywords)
        where_filter = {}

        if regions:
            where_filter['region'] = {'$in': regions}
        if topics:
            where_filter['topics'] = {'$in': topics}
        if date_range and 'days' in date_range:
            cutoff_date = (datetime.now() - timedelta(days=date_range['days'])).isoformat()
            where_filter['published_date'] = {'$gte': cutoff_date}

        # Vector search
        vector_results = self.chroma_client.query(
            query_texts=[query_text],
            n_results=k,
            where=where_filter if where_filter else None
        )

        # Keyword search (simplified - in production use BM25 library)
        # For PoC: filter vector results by keyword presence
        keyword_filtered = []
        for doc in vector_results['documents'][0]:
            if any(kw.lower() in doc.lower() for kw in keywords):
                keyword_filtered.append(doc)

        # RRF fusion (simplified)
        # In production: implement proper Reciprocal Rank Fusion
        return keyword_filtered[:k]

    def _format_articles(self, articles: list[dict]) -> str:
        """Format retrieved articles for LLM context."""
        formatted = []
        for i, article in enumerate(articles, 1):
            formatted.append(
                f"[{i}] {article['title']}\n"
                f"    Source: {article['source']} | Date: {article['published_date']}\n"
                f"    Summary: {article['summary']}\n"
                f"    URL: {article['url']}\n"
            )
        return "\n".join(formatted)
```

**Design Rationale**:
- **Query decomposition**: Converts natural language → structured filters (pattern from semantic search systems)
- **Hybrid retrieval**: BM25 (keyword) + vector search + RRF fusion (best practice from RAG systems)
- **ChainOfThought for generation**: Reasoning improves citation accuracy and trend detection
- **Modular design**: Each step (decompose, retrieve, generate) is independently testable

---

## 3. MODULE SELECTION RATIONALE

### 3.1 DSPy Module Decision Matrix

| Component | Module Choice | Alternative Considered | Rationale |
|-----------|---------------|------------------------|-----------|
| **Relevance Classifier** | `ChainOfThought` | `Predict` | CoT reasoning improves edge case handling (e.g., HR vs employment screening context). Worth the 2x latency cost for 60% filtering accuracy gain. |
| **Region Classifier** | `TypedPredictor` | `ChainOfThought` | Region is deterministic given entities. Type safety prevents hallucination. Faster inference (1.5s vs 3s). |
| **Topic Classifier** | `TypedPredictor` | `MultiChainProgram` | Multi-label output naturally fits TypedPredictor's structured output. MIPROv2 optimizer handles complexity. |
| **Hybrid Scorer** | Non-LLM (sklearn) | `Predict` | Cost optimization: TF-IDF + embeddings are 100x cheaper than LLM calls. No reasoning needed for scoring. |
| **Query Decomposer** | `ChainOfThought` | `ReAct` | Query parsing benefits from reasoning. ReAct overkill (no tools needed). CoT is faster. |
| **Answer Generator** | `ChainOfThought` | `ProgramOfThought` | PoT is for numerical/code tasks. CoT handles synthesis + citations well. |

### 3.2 Optimization Strategy per Module

**Pattern Applied**: Staged optimization with labeled data bootstrapping

```python
# Optimization Pipeline (Run once during setup, then periodically)

from dspy.teleprompt import BootstrapFewShot, MIPROv2
import dspy

# 1. Relevance Classifier Optimization
def optimize_relevance_classifier(labeled_articles: list[dspy.Example]):
    """
    Optimize using BootstrapFewShot with 50 hand-labeled examples.

    Labeled set:
    - 25 true positives (clearly relevant)
    - 15 true negatives (clearly irrelevant HR/employment news)
    - 10 edge cases (borderline, requires reasoning)
    """

    def relevance_metric(example, prediction, trace=None):
        """Binary accuracy + confidence calibration."""
        correct = (prediction.is_relevant == example.is_relevant)
        confidence_penalty = abs(prediction.confidence - (1.0 if correct else 0.0))
        return 1.0 if correct else -confidence_penalty

    optimizer = BootstrapFewShot(
        metric=relevance_metric,
        max_bootstrapped_demos=8,  # Use 8 best examples as few-shot demos
        max_labeled_demos=3         # Include 3 edge cases
    )

    classifier = RelevanceClassifier()
    optimized = optimizer.compile(
        classifier,
        trainset=labeled_articles[:40],   # 80% train
        valset=labeled_articles[40:]      # 20% validation
    )

    return optimized


# 2. Topic Classifier Optimization
def optimize_topic_classifier(labeled_articles: list[dspy.Example]):
    """
    Optimize using MIPROv2 for multi-label classification.

    Labeled set:
    - 100 articles with human-annotated topics
    - Balanced across 8 topic categories
    """

    def multi_label_f1(example, prediction, trace=None):
        """F1 score for multi-label classification."""
        true_topics = set(example.topics)
        pred_topics = set(prediction.topics)

        if not pred_topics:
            return 0.0

        precision = len(true_topics & pred_topics) / len(pred_topics)
        recall = len(true_topics & pred_topics) / len(true_topics)

        if precision + recall == 0:
            return 0.0

        f1 = 2 * (precision * recall) / (precision + recall)
        return f1

    optimizer = MIPROv2(
        metric=multi_label_f1,
        auto="medium",      # Medium optimization effort
        num_threads=4       # Parallel optimization
    )

    classifier = TopicClassifier()
    optimized = optimizer.compile(
        classifier,
        trainset=labeled_articles[:80],
        num_candidates=10,   # Generate 10 prompt candidates
        max_bootstrapped_demos=5,
        max_labeled_demos=3
    )

    return optimized


# 3. RAG Pipeline Optimization
def optimize_rag_pipeline(query_examples: list[dspy.Example]):
    """
    Optimize query decomposition + answer generation.

    Labeled set:
    - 30 query-answer pairs with human-written gold answers
    - Cover all regions and topics
    """

    def rag_quality_metric(example, prediction, trace=None):
        """Composite metric: answer relevance + citation accuracy."""
        # Use DSPy's semantic similarity
        answer_sim = dspy.evaluate.SemanticF1()(
            example.answer,
            prediction.answer
        )

        # Citation accuracy: are cited articles actually relevant?
        citation_score = len(set(example.citations) & set(prediction.citations)) / \
                        max(len(example.citations), 1)

        return 0.7 * answer_sim + 0.3 * citation_score

    optimizer = BootstrapFewShot(
        metric=rag_quality_metric,
        max_bootstrapped_demos=5
    )

    rag = NewsletterRAG(chroma_client, json_store_path)
    optimized = optimizer.compile(
        rag,
        trainset=query_examples[:24],
        valset=query_examples[24:]
    )

    return optimized
```

**Optimization Schedule**:
- **Initial Setup**: Hand-label 50 relevance examples + 100 topic examples + 30 query pairs
- **Bootstrap Phase**: Run optimizers to generate few-shot demos
- **Monthly Re-optimization**: Re-run as new labeled data accumulates from human review feedback

---

## 4. RAG IMPLEMENTATION STRATEGY

### 4.1 Vector Database Architecture (ChromaDB)

**Design Pattern**: Hierarchical collections with metadata filtering

```python
import chromadb
from chromadb.config import Settings

class NewsletterVectorStore:
    """
    ChromaDB-based vector store for newsletter articles.
    """

    def __init__(self, persist_directory: str = "./chroma_db"):
        self.client = chromadb.Client(Settings(
            chroma_db_impl="duckdb+parquet",
            persist_directory=persist_directory
        ))

        # Create collections by region for faster filtering
        self.collections = {}
        for region in ["africa_middle_east", "asia_pacific", "europe",
                      "north_america_caribbean", "south_america", "worldwide"]:
            self.collections[region] = self.client.get_or_create_collection(
                name=f"articles_{region}",
                metadata={"hnsw:space": "cosine"},  # Cosine similarity
                embedding_function=None  # We'll provide embeddings directly
            )

    def add_article(self, article: dict, embedding: list[float]):
        """
        Add article to appropriate regional collection.

        Args:
            article: dict with keys: id, title, content, region, topics,
                    published_date, source, url, relevance_score
            embedding: 1536-dim vector from text-embedding-3-small
        """
        region = article['region']

        self.collections[region].add(
            ids=[article['id']],
            embeddings=[embedding],
            metadatas=[{
                'title': article['title'],
                'region': article['region'],
                'topics': ','.join(article['topics']),  # Comma-separated for filtering
                'published_date': article['published_date'],
                'source': article['source'],
                'url': article['url'],
                'relevance_score': article['relevance_score']
            }],
            documents=[f"{article['title']} {article.get('summary', '')}"]
        )

    def query(self, query_embedding: list[float], regions: list[str] = None,
              topics: list[str] = None, k: int = 20, date_filter: str = None):
        """
        Hybrid query across collections.

        Args:
            query_embedding: Query vector
            regions: List of regions to search (None = all)
            topics: List of topics to filter by
            k: Number of results
            date_filter: ISO date string (return articles >= this date)

        Returns:
            List of article metadata dicts
        """
        search_regions = regions if regions else list(self.collections.keys())

        all_results = []

        for region in search_regions:
            where_filter = {}

            # Topic filter (articles with ANY of the specified topics)
            if topics:
                # ChromaDB supports $or for multiple conditions
                topic_conditions = [{'topics': {'$contains': topic}} for topic in topics]
                where_filter['$or'] = topic_conditions

            # Date filter
            if date_filter:
                where_filter['published_date'] = {'$gte': date_filter}

            results = self.collections[region].query(
                query_embeddings=[query_embedding],
                n_results=k,
                where=where_filter if where_filter else None,
                include=['metadatas', 'documents', 'distances']
            )

            # Convert to standardized format
            for i in range(len(results['ids'][0])):
                all_results.append({
                    'id': results['ids'][0][i],
                    'metadata': results['metadatas'][0][i],
                    'distance': results['distances'][0][i],
                    'relevance': 1 - results['distances'][0][i]  # Convert distance to similarity
                })

        # Sort by relevance and return top K
        all_results.sort(key=lambda x: x['relevance'], reverse=True)
        return all_results[:k]

    def get_article_by_id(self, article_id: str) -> dict:
        """Retrieve full article from JSON store by ID."""
        # This would load from JSON files (implementation in storage layer)
        pass
```

**Storage Strategy**:
- **Hot data** (last 90 days): ChromaDB + in-memory cache
- **Warm data** (90-365 days): ChromaDB only
- **Cold data** (>365 days): JSON archive, load on-demand for queries

**Index Configuration**:
- **HNSW parameters**: M=16, efConstruction=200 (balanced speed/recall)
- **Embedding dimension**: 1536 (OpenAI text-embedding-3-small)
- **Distance metric**: Cosine similarity

### 4.2 Embedding Generation Pipeline

```python
import openai
from typing import List
import numpy as np

class EmbeddingGenerator:
    """
    Async embedding generation with batching and caching.
    """

    def __init__(self, model: str = "text-embedding-3-small", batch_size: int = 100):
        self.model = model
        self.batch_size = batch_size
        self.cache = {}  # In-memory cache (in production: Redis)

    def generate_embeddings(self, articles: List[dict]) -> dict[str, np.ndarray]:
        """
        Generate embeddings for batch of articles.

        Args:
            articles: List of article dicts with 'id', 'title', 'summary'

        Returns:
            Dict mapping article_id -> embedding vector
        """
        embeddings = {}

        # Prepare texts (title + summary, max 512 tokens)
        texts_to_embed = []
        article_ids = []

        for article in articles:
            text = f"{article['title']} {article.get('summary', '')}"
            text = text[:2048]  # Truncate to ~512 tokens

            # Check cache
            cache_key = self._hash_text(text)
            if cache_key in self.cache:
                embeddings[article['id']] = self.cache[cache_key]
            else:
                texts_to_embed.append(text)
                article_ids.append(article['id'])

        # Batch API calls
        if texts_to_embed:
            for i in range(0, len(texts_to_embed), self.batch_size):
                batch_texts = texts_to_embed[i:i+self.batch_size]
                batch_ids = article_ids[i:i+self.batch_size]

                response = openai.embeddings.create(
                    model=self.model,
                    input=batch_texts
                )

                for j, embedding_obj in enumerate(response.data):
                    article_id = batch_ids[j]
                    embedding = np.array(embedding_obj.embedding)

                    embeddings[article_id] = embedding

                    # Cache
                    cache_key = self._hash_text(batch_texts[j])
                    self.cache[cache_key] = embedding

        return embeddings

    def _hash_text(self, text: str) -> str:
        """Simple hash for caching."""
        import hashlib
        return hashlib.md5(text.encode()).hexdigest()

    def generate_topic_embeddings(self, topic_descriptions: dict[str, str]) -> dict[str, np.ndarray]:
        """
        Pre-compute embeddings for each topic category.

        Args:
            topic_descriptions: Dict mapping topic name -> description

        Example:
            {
                'regulatory_legal': 'Employment law changes, compliance requirements, FCRA/GDPR updates',
                'criminal_background': 'Criminal record checks, background screening regulations'
            }

        Returns:
            Dict mapping topic -> embedding
        """
        topic_embeddings = {}

        descriptions = list(topic_descriptions.values())
        topic_names = list(topic_descriptions.keys())

        response = openai.embeddings.create(
            model=self.model,
            input=descriptions
        )

        for i, embedding_obj in enumerate(response.data):
            topic_embeddings[topic_names[i]] = np.array(embedding_obj.embedding)

        return topic_embeddings
```

**Cost Optimization**:
- **Batch API calls**: 100 articles per request reduces overhead
- **Caching**: Hash-based dedup saves ~10% on repeat content
- **Model choice**: text-embedding-3-small ($0.02/1M tokens) vs ada-002 ($0.10/1M tokens)
- **Estimated cost**: 225 articles/day × 150 tokens/article × $0.02/1M = $0.68/month

### 4.3 Query Processing Flow

```
User Query: "Ban the Box updates in California last 30 days"
│
├─> QueryDecomposer (DSPy ChainOfThought)
│   Output: {
│     regions: ['north_america_caribbean'],
│     topics: ['regulatory_legal'],
│     keywords: ['Ban the Box', 'California', 'fair chance'],
│     date_range: {days: 30}
│   }
│
├─> Generate Query Embedding (OpenAI API)
│   Input: "Ban the Box California fair chance"
│   Output: [1536-dim vector]
│
├─> HybridRetriever
│   ├─> Vector Search (ChromaDB)
│   │   - Collection: articles_north_america_caribbean
│   │   - Where: topics CONTAINS 'regulatory_legal' AND date >= 2025-12-13
│   │   - Top 20 by cosine similarity
│   │
│   ├─> Keyword Filter
│   │   - Filter vector results by keyword presence
│   │   - Boost articles with "Ban the Box" in title
│   │
│   └─> RRF Fusion
│       - Merge vector + keyword rankings
│       - Return top 10
│
├─> Load Full Articles (JSON Store)
│   - Retrieve summaries, metadata for top 10
│
└─> AnswerGenerator (DSPy ChainOfThought)
    Input: Query + 10 articles context
    Output: {
      answer: "Three recent Ban the Box updates in California...",
      citations: ["California DFEH Guidance (2025-12-20)", ...],
      trend_summary: "Trend toward stricter compliance enforcement..."
    }
```

**Performance Targets**:
- Query decomposition: <2s
- Vector search: <1s (HNSW with M=16)
- Answer generation: <5s
- **Total latency**: <8s (well under 10s SLA)

---

## 5. PHASED IMPLEMENTATION PLAN

### Phase 0: Foundation & Setup (Week 1)

**Goal**: Environment setup, data collection, baseline tooling

**Deliverables**:
1. Python environment with DSPy, ChromaDB, OpenAI SDK
2. RSS feed fetcher for Tier 1 sources (20 feeds)
3. Basic HTML parser (BeautifulSoup/newspaper3k)
4. JSON storage structure (`articles/YYYY-MM-DD.json`)
5. 50 hand-labeled relevance examples
6. 100 hand-labeled topic classification examples

**Success Criteria**:
- Fetch 500+ articles from Tier 1 sources
- Parse 95%+ successfully (title, content, date, source)
- Store in JSON with consistent schema

**Effort**: 3-4 days

---

### Phase 1: Core Classification Pipeline (Week 2-3)

**Goal**: Implement and optimize DSPy classifiers

**Tasks**:

1. **Relevance Classifier** (3 days)
   - Implement `RelevanceSignature` + `RelevanceClassifier`
   - Keyword extraction pre-processor (regex + keyword list)
   - Optimize with BootstrapFewShot on 50 labeled examples
   - Target: 70%+ recall, <30% false positive rate

2. **Region Classifier** (2 days)
   - Implement `RegionSignature` + `RegionClassifier`
   - Add spaCy NER pipeline for entity extraction
   - Test on 30 geographically diverse articles
   - Target: 85%+ accuracy

3. **Topic Classifier** (3 days)
   - Implement `TopicSignature` + `TopicClassifier`
   - Optimize with MIPROv2 on 100 labeled examples
   - Multi-label evaluation (F1 per topic + macro F1)
   - Target: 0.75+ macro F1

4. **Pipeline Integration** (2 days)
   - Chain classifiers: Relevance → Region → Topics
   - Add logging and error handling
   - Process 7 days of historical articles (~3,500 total)
   - Generate classification performance report

**Deliverables**:
- Optimized DSPy modules saved to `models/`
- Classification pipeline script (`classify_articles.py`)
- Performance report (precision/recall/F1 per module)

**Success Criteria**:
- Relevance classifier: 70%+ recall vs manual process
- Topic classifier: 0.75+ F1 score
- Pipeline processes 500 articles in <45 minutes

**Effort**: 10 days

---

### Phase 2: Deduplication & Scoring (Week 4)

**Goal**: Reduce noise and rank articles by relevance

**Tasks**:

1. **Deduplication** (3 days)
   - URL normalization (remove tracking params)
   - SHA-256 content hashing for exact dedup
   - SimHash implementation (64-bit, 3-word shingles)
   - LSH bucketing for near-duplicate detection
   - Test on known duplicate sets (syndicated articles)

2. **Hybrid Relevance Scorer** (3 days)
   - TF-IDF vectorizer with 50+ keywords
   - Generate embeddings for all articles (batch process)
   - Pre-compute topic embeddings (8 topics)
   - Implement scoring formula (0.4 keyword + 0.6 semantic + freshness)
   - Validate scores vs human rankings (Spearman correlation)

3. **Daily Shortlist Generator** (1 day)
   - Rank by relevance score
   - Apply diversity filters (max 3 per source)
   - Generate top 20-50 list
   - Export to CSV for human review

**Deliverables**:
- Deduplication module (`dedup.py`)
- Hybrid scorer (`scoring.py`)
- Daily shortlist script (`generate_shortlist.py`)
- Validation report (dedup precision, scoring correlation)

**Success Criteria**:
- Dedup reduces article count by 20-25%
- Hybrid scores correlate 0.7+ with human rankings
- Daily shortlist contains 70%+ of manually selected articles

**Effort**: 7 days

---

### Phase 3: Vector Store & RAG (Week 5-6)

**Goal**: Enable natural language querying over historical corpus

**Tasks**:

1. **ChromaDB Setup** (2 days)
   - Create regional collections (6 collections)
   - Index 30 days of historical articles (~7,500 total)
   - Configure HNSW (M=16, efConstruction=200)
   - Benchmark query latency (<1s target)

2. **Embedding Pipeline** (2 days)
   - Batch embedding generation (100 articles/call)
   - Implement caching (Redis or in-memory)
   - Generate topic embeddings
   - Store embeddings in ChromaDB

3. **Query Decomposition** (2 days)
   - Implement `QueryDecompositionSignature`
   - Test on 30 example queries
   - Optimize with BootstrapFewShot
   - Target: 90%+ correct filter extraction

4. **Hybrid Retrieval** (3 days)
   - ChromaDB vector search
   - BM25 keyword search (rank-bm25 library)
   - Reciprocal Rank Fusion (RRF)
   - Validate retrieval quality (NDCG@10)

5. **Answer Generation** (3 days)
   - Implement `AnswerGenerationSignature`
   - Format retrieved articles for context
   - Optimize with BootstrapFewShot on 30 query-answer pairs
   - Add citation extraction and validation

6. **CLI Interface** (2 days)
   - Build interactive CLI (Click or Typer)
   - Commands: `query`, `list-recent`, `get-article`, `stats`
   - Add query history and result caching

**Deliverables**:
- Vector store setup script (`setup_vectorstore.py`)
- RAG module (`rag.py`)
- CLI tool (`newsletter_cli.py`)
- Evaluation report (retrieval NDCG, answer quality)

**Success Criteria**:
- Query latency <8s (end-to-end)
- Retrieval NDCG@10 >0.7
- Answer quality rated 4+/5 by domain expert on 20 test queries

**Effort**: 14 days

---

### Phase 4: Production Pipeline & Monitoring (Week 7)

**Goal**: Automate daily batch processing and establish monitoring

**Tasks**:

1. **End-to-End Pipeline** (3 days)
   - Orchestrate: Ingest → Dedup → Classify → Score → Embed → Store
   - Add error handling and retry logic
   - Implement idempotency (skip already-processed articles)
   - Schedule with cron (daily 2 AM UTC run)

2. **Monitoring Dashboard** (2 days)
   - Metrics: articles processed, dedup rate, classification distribution, avg scores
   - Alerts: fetch failures, classification errors, low relevance day
   - Simple web dashboard (Streamlit or Gradio)

3. **Human Review Integration** (2 days)
   - Export daily shortlist to Google Sheets via API
   - Collect feedback: thumbs up/down, topic corrections
   - Store feedback for future re-optimization

4. **Documentation** (1 day)
   - System architecture diagram
   - Operator runbook (how to run pipeline, troubleshoot)
   - API/CLI reference
   - Model re-training guide

**Deliverables**:
- Production pipeline script (`run_daily_pipeline.py`)
- Monitoring dashboard (`dashboard.py`)
- Human review integration (`export_to_sheets.py`)
- Documentation (`docs/`)

**Success Criteria**:
- Pipeline runs successfully for 7 consecutive days
- Dashboard shows real-time metrics
- Human feedback collected for 50+ articles

**Effort**: 8 days

---

### Phase 5 (Future): Scraping & Multi-Language (Post-PoC)

**Scope** (not part of initial PoC):
- Web scraping for Tier 3 sources (PBSA, DPAs)
- Email parsing integration (IMAP + parsing libraries)
- Multi-language support (non-English content)
- Real-time monitoring (WebSocket/streaming ingestion)
- Personalized feeds (user profiles, collaborative filtering)

---

## 6. RISKS & MITIGATIONS

### 6.1 Technical Risks

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| **LLM API costs exceed budget** | Medium | High | 1) Use smaller models where possible (TypedPredictor vs CoT for region/topic)<br>2) Batch API calls<br>3) Cache embeddings<br>4) Set monthly spending cap ($100 for PoC) |
| **Classification accuracy <70% recall** | Medium | High | 1) Increase labeled training set (50→100 examples)<br>2) Try stronger model (GPT-4 vs GPT-3.5)<br>3) Add keyword pre-filtering stage<br>4) Iterate on signature descriptions |
| **Deduplication misses syndicated content** | Low | Medium | 1) Lower SimHash threshold (3→5 Hamming distance)<br>2) Add cross-source clustering (embed + cluster articles within 24hr window)<br>3) Manual review of dedup failures |
| **Query latency >10s** | Low | Medium | 1) Pre-compute embeddings offline<br>2) Use smaller ChromaDB collections (regional split)<br>3) Cache common queries<br>4) Reduce K in retrieval (20→10) |
| **ChromaDB index growth (storage)** | Low | Low | 1) TTL policy: archive articles >365 days<br>2) Use quantization (1536→384 dims with distillation)<br>3) Periodic compaction |

### 6.2 Data Quality Risks

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| **RSS feeds go stale/broken** | High | Medium | 1) Daily fetch monitoring with alerts<br>2) Maintain backup sources per topic<br>3) Weekly manual audit of source health |
| **HTML parsing failures** | Medium | Medium | 1) Use multiple parsers (BeautifulSoup, newspaper3k, Goose3)<br>2) Fallback to raw text extraction<br>3) Log failures for manual review |
| **Non-English content mis-classified** | Medium | Low | 1) Add language detection (langdetect library)<br>2) Route non-English to separate pipeline or skip for PoC<br>3) Phase 5: add translation layer |
| **Spam/promotional content passes filter** | Low | Medium | 1) Add domain reputation scoring<br>2) Keyword blacklist (e.g., "webinar registration", "free trial")<br>3) Human review feedback loop |

### 6.3 Operational Risks

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| **Pipeline crashes mid-run** | Low | Medium | 1) Idempotency: track processed article IDs<br>2) Checkpoint after each stage<br>3) Auto-retry with exponential backoff |
| **Human reviewers overwhelmed** | Medium | High | 1) Tune relevance threshold to reduce shortlist size<br>2) Prioritize by score (review top 20, defer 21-50)<br>3) Add "auto-approve" for high-confidence articles |
| **Model drift over time** | Medium | Medium | 1) Monthly re-optimization with new labeled data<br>2) Track accuracy metrics in dashboard<br>3) Alert on >10% accuracy drop |
| **Dependency on external APIs** | Medium | High | 1) OpenAI API: fallback to Anthropic/local model<br>2) Rate limiting + retry logic<br>3) Cache embeddings aggressively |

### 6.4 Success Criteria Risks

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| **70% recall not achieved** | Medium | High | 1) Lower threshold to 60% for MVP<br>2) Focus on high-value topics first (regulatory, criminal)<br>3) Hybrid human-AI workflow (AI suggests, human decides) |
| **False positive rate too high** | Low | Medium | 1) Increase confidence threshold for auto-inclusion<br>2) Add second-stage human review<br>3) Iteratively refine relevance signature |
| **Query responses lack citations** | Low | Low | 1) Add citation requirement to AnswerGenerationSignature<br>2) Post-process to extract URLs<br>3) Penalize answers without citations in optimization metric |

---

## 7. COST ANALYSIS & BUDGET

### 7.1 API Cost Breakdown (Monthly)

**OpenAI API Usage**:

| Operation | Volume | Model | Cost per 1M tokens | Monthly Cost |
|-----------|--------|-------|---------------------|--------------|
| **Relevance Classification** | 375 articles/day × 600 tokens | GPT-4o-mini | $0.150 input | $1.01 |
| **Region Classification** | 225 articles/day × 2000 tokens | GPT-4o-mini | $0.150 input | $2.03 |
| **Topic Classification** | 225 articles/day × 2000 tokens | GPT-4o-mini | $0.150 input | $2.03 |
| **Embeddings** | 225 articles/day × 150 tokens | text-embedding-3-small | $0.020 | $0.68 |
| **Query Decomposition** | 10 queries/day × 200 tokens | GPT-4o-mini | $0.150 input | $0.09 |
| **Answer Generation** | 10 queries/day × 5000 tokens | GPT-4o-mini | $0.600 output | $0.90 |
| **Optimization (monthly)** | 200 examples × 3000 tokens | GPT-4o-mini | $0.150 input | $0.09 |

**Total Monthly API Cost**: ~$7/month

**Annual**: ~$84/year

### 7.2 Infrastructure Costs

- **Compute**: Local laptop or $5/month VPS (1 CPU, 2GB RAM sufficient for daily batch)
- **Storage**:
  - JSON: 50MB/day × 30 = 1.5GB/month → ~$0.03/month (S3)
  - ChromaDB: 2GB index → local storage or $0.05/month (S3)
- **Total Infrastructure**: <$10/month

### 7.3 Development Time (PoC)

- **Architect**: 2 days (design)
- **Developer**: 7 weeks × 5 days = 35 days (implementation)
- **Domain Expert**: 5 days (labeling, review, feedback)

**Total Effort**: ~42 person-days

### 7.4 Cost vs Manual Process

**Current Manual Process**:
- 20 hours/week × 4 weeks = 80 hours/month
- At $50/hour → $4,000/month
- Annual: $48,000

**Automated System**:
- API + Infrastructure: $17/month
- Maintenance: 2 hours/week × $50/hour = $400/month
- Annual: $5,000

**Savings**: $43,000/year (90% reduction)

---

## 8. EVALUATION METRICS & SUCCESS TRACKING

### 8.1 PoC Success Criteria

| Metric | Target | Measurement Method |
|--------|--------|-------------------|
| **Recall** | 70%+ | Compare automated shortlist vs manual process over 2 weeks |
| **Precision** | 50%+ | % of shortlist articles rated "relevant" by human reviewer |
| **Processing Time** | <2 hours/day | Pipeline execution logs |
| **Query Latency** | <8s | Average response time over 50 test queries |
| **Retrieval Quality** | NDCG@10 >0.7 | Evaluate on 30 query-answer test set |
| **Cost** | <$20/month | Track OpenAI API usage |

### 8.2 Ongoing Monitoring Metrics

**Daily Metrics**:
- Articles fetched (target: 500+)
- Dedup rate (expected: 20-25%)
- Relevance pass rate (expected: 60%)
- Classification distribution (region/topic balance)
- Avg relevance score (track drift over time)

**Weekly Metrics**:
- Human review approval rate (target: 70%+)
- False positive rate (target: <30%)
- Source health (# of failed fetches)

**Monthly Metrics**:
- Recall vs manual process
- Query usage patterns
- Model accuracy (if re-evaluated on new labeled data)

### 8.3 Dashboard Visualizations

```
┌─────────────────────────────────────────────────────────────┐
│  Newsletter Research Pipeline - Daily Dashboard             │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  📊 Processing Stats (Last 24h)                              │
│  ├─ Articles Fetched: 512                                    │
│  ├─ Unique (after dedup): 387 (24% reduction)               │
│  ├─ Relevant: 235 (61% pass rate)                           │
│  └─ Shortlist Size: 42 articles                             │
│                                                              │
│  🌍 Regional Distribution                                    │
│  ├─ North America: 45% ████████████                          │
│  ├─ Europe: 30% ████████                                     │
│  ├─ Asia Pacific: 15% ████                                   │
│  └─ Other: 10% ██                                            │
│                                                              │
│  📂 Topic Distribution                                       │
│  ├─ Regulatory/Legal: 38% █████████                          │
│  ├─ Criminal Background: 22% █████                           │
│  ├─ Tech/Products: 15% ███                                   │
│  └─ Other: 25% ██████                                        │
│                                                              │
│  💰 API Costs (MTD)                                          │
│  ├─ Classification: $3.12                                    │
│  ├─ Embeddings: $0.45                                        │
│  ├─ Queries: $0.23                                           │
│  └─ Total: $3.80 / $20.00 budget                             │
│                                                              │
│  ⚠️  Alerts                                                  │
│  └─ No issues detected                                       │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

---

## 9. HANDOFF TO IMPLEMENTATION

### 9.1 Next Steps for Development Team

1. **Immediate (Day 1)**:
   - Review this document with solution architect
   - Set up Python environment (see `requirements.txt` below)
   - Clone starter code repository (if available)
   - Schedule kickoff with domain expert for labeling session

2. **Week 1**:
   - Execute Phase 0 (Foundation & Setup)
   - Collect and store 7 days of historical articles
   - Begin hand-labeling 50 relevance + 100 topic examples

3. **Week 2-3**:
   - Implement Phase 1 (Classification Pipeline)
   - Daily standups to review classification accuracy
   - Iterate on signatures based on errors

4. **Week 4**:
   - Implement Phase 2 (Deduplication & Scoring)
   - Begin collecting human review feedback

5. **Week 5-6**:
   - Implement Phase 3 (Vector Store & RAG)
   - User testing of CLI with domain experts

6. **Week 7**:
   - Implement Phase 4 (Production Pipeline)
   - Launch pilot with daily automated runs

### 9.2 Recommended Tech Stack

```txt
# requirements.txt

# Core
python>=3.10
dspy-ai>=2.5.0

# LLM APIs
openai>=1.0.0
anthropic>=0.8.0  # Backup LLM provider

# Vector DB
chromadb>=0.4.0

# NLP & ML
spacy>=3.7.0
scikit-learn>=1.3.0
rank-bm25>=0.2.2
langdetect>=1.0.9

# Web Scraping & Parsing
feedparser>=6.0.10
beautifulsoup4>=4.12.0
newspaper3k>=0.2.8
requests>=2.31.0

# Deduplication
simhash>=2.1.0

# Data
pandas>=2.0.0
numpy>=1.24.0

# CLI
click>=8.1.0
rich>=13.0.0  # Pretty terminal output

# Monitoring
streamlit>=1.28.0  # Dashboard

# Testing
pytest>=7.4.0
pytest-asyncio>=0.21.0
```

### 9.3 Repository Structure

```
dspy-newsletter-research/
├── README.md
├── requirements.txt
├── .env.example
├── config/
│   ├── sources.yaml          # RSS feeds, scraping targets
│   ├── keywords.yaml          # Relevance keywords
│   └── taxonomy.yaml          # Regions, topics definitions
├── data/
│   ├── articles/              # JSON storage (YYYY-MM-DD.json)
│   ├── labeled/               # Hand-labeled training examples
│   ├── embeddings/            # Cached embeddings
│   └── chroma_db/             # ChromaDB persistence
├── models/
│   ├── relevance_classifier/  # Optimized DSPy modules
│   ├── region_classifier/
│   ├── topic_classifier/
│   └── rag_pipeline/
├── src/
│   ├── ingestion/
│   │   ├── rss_fetcher.py
│   │   ├── web_scraper.py
│   │   └── parser.py
│   ├── deduplication/
│   │   ├── exact_dedup.py
│   │   └── simhash_dedup.py
│   ├── classification/
│   │   ├── relevance.py       # RelevanceClassifier
│   │   ├── region.py          # RegionClassifier
│   │   ├── topic.py           # TopicClassifier
│   │   └── scoring.py         # HybridRelevanceScorer
│   ├── embeddings/
│   │   └── generator.py       # EmbeddingGenerator
│   ├── vectorstore/
│   │   └── chroma_store.py    # NewsletterVectorStore
│   ├── rag/
│   │   ├── query_decomposer.py
│   │   ├── retriever.py
│   │   └── answer_generator.py
│   ├── pipeline/
│   │   └── orchestrator.py    # End-to-end pipeline
│   └── utils/
│       ├── logging.py
│       └── metrics.py
├── scripts/
│   ├── setup_vectorstore.py
│   ├── run_daily_pipeline.py
│   ├── generate_shortlist.py
│   └── optimize_models.py
├── cli/
│   └── newsletter_cli.py      # Click-based CLI
├── dashboard/
│   └── app.py                 # Streamlit dashboard
├── tests/
│   ├── test_classification.py
│   ├── test_dedup.py
│   └── test_rag.py
└── docs/
    ├── architecture.md
    ├── runbook.md
    └── api_reference.md
```

### 9.4 Key Integration Points

**With Existing Systems**:
- **Dropbox**: Replace manual dumps with automated JSON exports (Phase 4)
- **Email Subscriptions**: IMAP parsing integration (Future phase)
- **Google Sheets**: Export daily shortlist via Sheets API for human review

**With Future Systems**:
- **Personalization Engine**: User profile modeling for customized feeds
- **Real-Time Alerts**: WebSocket/SSE for breaking news push notifications
- **Multi-Language Pipeline**: Translation layer for non-English sources

---

## 10. APPENDIX

### 10.1 Pattern Recognition Applied

This architecture applies proven patterns from:

1. **Google News / Flipboard**:
   - Multi-stage event-driven pipeline (ingestion → classification → ranking)
   - Hybrid keyword + semantic scoring
   - Story clustering for deduplication
   - Regional collections for faster filtering

2. **Modern RAG Systems** (LlamaIndex, LangChain):
   - Query decomposition for structured retrieval
   - Reciprocal Rank Fusion (RRF) for hybrid search
   - ChainOfThought for answer generation with citations
   - Metadata filtering in vector search

3. **Content Classification Platforms** (Feedly, Inoreader):
   - Multi-label topic classification
   - Relevance filtering before heavy processing
   - Freshness-weighted scoring
   - Human-in-the-loop feedback integration

4. **DSPy Best Practices**:
   - TypedPredictor for structured outputs (regions, topics)
   - ChainOfThought for reasoning tasks (relevance, answers)
   - BootstrapFewShot for small labeled datasets (<100 examples)
   - MIPROv2 for complex multi-label tasks
   - Modular signatures (composable, testable)

### 10.2 Alternative Approaches Considered

| Alternative | Why Not Chosen |
|-------------|----------------|
| **Fine-tuned classifier model** | Requires 1000+ labeled examples (we have 100). DSPy optimization achieves comparable accuracy with 10x less data. |
| **Real-time streaming pipeline** | Daily batch is sufficient for newsletter use case. Streaming adds complexity without business value for PoC. |
| **Pinecone/Weaviate vector DB** | ChromaDB is free, local, and sufficient for 10K articles/month. Can migrate to managed DB if scale exceeds 100K articles. |
| **GPT-4 for all classification** | Cost prohibitive ($30+/month vs $7/month). GPT-4o-mini + optimization achieves 95% of GPT-4 accuracy at 20% cost. |
| **Separate dedup after classification** | Dedup before classification saves 25% of API calls. Syndicated content doesn't need to be classified multiple times. |
| **Elasticsearch for search** | BM25 library + ChromaDB is simpler for PoC. Elasticsearch overkill for 10K documents (useful at 1M+ scale). |

### 10.3 Glossary

- **BootstrapFewShot**: DSPy optimizer that generates few-shot examples from labeled data
- **ChainOfThought (CoT)**: DSPy module that adds reasoning steps before output
- **ChromaDB**: Open-source vector database with HNSW indexing
- **HNSW**: Hierarchical Navigable Small World graph, ANN algorithm for vector search
- **MIPROv2**: DSPy optimizer for multi-stage pipelines (prompt + demonstration optimization)
- **NDCG**: Normalized Discounted Cumulative Gain, metric for ranking quality
- **RRF**: Reciprocal Rank Fusion, algorithm for merging ranked lists
- **SimHash**: Locality-sensitive hashing algorithm for near-duplicate detection
- **TypedPredictor**: DSPy module that enforces type-safe structured outputs

### 10.4 References

- **DSPy Documentation**: https://dspy-docs.vercel.app
- **ChromaDB Docs**: https://docs.trychroma.com
- **News Aggregation Architecture**: AlgoMaster System Design (Perplexity source [5])
- **Hybrid Scoring**: Google News System Design (Perplexity source [1])
- **RAG Patterns**: DSPy tutorials (Context7 sources)

---

**END OF SOLUTION DESIGN DOCUMENT**

---

## Document Metadata

**Created By**: Architect 2 (Pattern Recognition Strategy)
**Review Status**: Ready for Engineering Review
**Approval Required**: Project Manager, Lead Developer, Domain Expert
**Estimated Implementation**: 7 weeks (35 person-days)
**Budget**: $7/month (PoC), $17/month (production)
**Next Review Date**: After Phase 1 completion (Week 3)

---

*This document is a living artifact and should be updated as implementation reveals new insights or constraints.*
