{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Project & Infrastructure Setup (Python, DSPy, Weaviate Docker)",
        "description": "Initialize the Python project, configure dependencies, environment variables, and bring up a local Weaviate instance via Docker Compose with the specified schema.",
        "details": "Implementation details:\n- Use Python 3.11+ and a modern dependency manager (e.g., **uv** or **poetry**) to create the project:\n  - Example (uv): `uv init dspy_newsletter_tool` then add dependencies listed in PRD to `pyproject.toml`.\n- Install core libs: `dspy-ai>=2.5.0`, `retrieve-dspy>=0.1.0`, `weaviate-client>=4.0.0`, `openai>=1.0.0`, `cohere>=5.0.0`, `feedparser>=6.0.0`, `beautifulsoup4>=4.12.0`, `playwright>=1.40.0`, `click>=8.1.0`, `rich>=13.0.0`, `pydantic>=2.0.0`, `python-dotenv>=1.0.0`.\n- Create `.env` and load using `python-dotenv`:\n  - `OPENAI_API_KEY`, `COHERE_API_KEY`, `WEAVIATE_URL=http://localhost:8080`.\n- Add the given `docker-compose.yml` to repo and adjust only if needed for local paths; keep `text2vec-openai` and `reranker-cohere` modules enabled exactly as specified.\n- Run `docker compose up -d weaviate` and ensure ports 8080/50051 are available.\n- Implement a small `scripts/init_weaviate.py` that:\n  - Connects using `weaviate.connect_to_local()` (or `weaviate.WeaviateClient(WEAVIATE_URL)` for latest client patterns).\n  - Checks if `NewsletterArticles` collection exists; if not, creates it with the exact schema shown in the PRD (properties, vectorizer, reranker config).\n- Configure DSPy globally in a `config/dspy_config.py` module:\n  ```python\n  import dspy, os\n  from dotenv import load_dotenv\n\n  load_dotenv()\n\n  lm = dspy.LM(\"openai/gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n  dspy.configure(lm=lm)\n  ```\n- Add basic health check script `scripts/health_check.py`:\n  - Verifies Weaviate connectivity by listing collections.\n  - Performs a minimal DSPy `dspy.Predict` call with a dummy signature to confirm OpenAI key works.\n\nSecurity & best practices:\n- Never hardcode API keys; rely on `.env` and environment variables.\n- Add `.env` and any local volumes to `.gitignore`.\n- Use separate virtualenv per project.\n\nPseudocode (schema init):\n```python\n# scripts/init_weaviate.py\nfrom dotenv import load_dotenv\nimport os, weaviate\nfrom weaviate.classes.config import Configure, Property, DataType\n\nload_dotenv()\nclient = weaviate.connect_to_local()\n\nif \"NewsletterArticles\" not in [c.name for c in client.collections.list()]:\n    client.collections.create(\n        name=\"NewsletterArticles\",\n        vectorizer_config=Configure.Vectorizer.text2vec_openai(\n            model=\"text-embedding-3-small\"\n        ),\n        reranker_config=Configure.Reranker.cohere(\n            model=\"rerank-english-v3.0\"\n        ),\n        properties=[\n            Property(name=\"title\", data_type=DataType.TEXT),\n            Property(name=\"content\", data_type=DataType.TEXT),\n            Property(name=\"summary\", data_type=DataType.TEXT),\n            Property(name=\"url\", data_type=DataType.TEXT),\n            Property(name=\"source\", data_type=DataType.TEXT),\n            Property(name=\"region\", data_type=DataType.TEXT, skip_vectorization=True),\n            Property(name=\"country\", data_type=DataType.TEXT, skip_vectorization=True),\n            Property(name=\"topics\", data_type=DataType.TEXT_ARRAY, skip_vectorization=True),\n            Property(name=\"relevance_score\", data_type=DataType.NUMBER, skip_vectorization=True),\n            Property(name=\"reasoning\", data_type=DataType.TEXT),\n            Property(name=\"published_date\", data_type=DataType.DATE, skip_vectorization=True),\n            Property(name=\"ingested_at\", data_type=DataType.DATE, skip_vectorization=True),\n        ],\n    )\n\nclient.close()\n```",
        "testStrategy": "- Run `docker compose up -d weaviate` and verify container is healthy via `docker ps` and hitting `http://localhost:8080/v1/meta`.\n- Execute `python scripts/init_weaviate.py` and then use a short script to list collections and confirm `NewsletterArticles` exists with expected properties.\n- Run a basic DSPy smoke test:\n  - Define a trivial `dspy.Signature` and `dspy.Predict` module; call it and confirm a non-error response.\n- Add a `pytest` that:\n  - Imports the Weaviate client, pings the server, and asserts connectivity.\n  - Ensures environment variables are loaded and non-empty for required keys.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Core Data Models, Taxonomy, and Processing Log Schema",
        "description": "Define Python data models for articles, taxonomy (regions, topics), and the JSONL processing audit log, ensuring consistent structured data throughout the pipeline.",
        "details": "Implementation details:\n- Create a `models/` package with Pydantic v2 models for strong typing and validation.\n\nArticle model:\n```python\nfrom pydantic import BaseModel, HttpUrl, Field\nfrom datetime import datetime\nfrom typing import List, Literal, Optional\n\nRegionCode = Literal[\n    \"africa_me\", \"asia_pacific\", \"europe\",\n    \"north_america\", \"south_america\", \"worldwide\",\n]\n\nTopicCode = Literal[\n    \"regulatory\", \"criminal_records\", \"education\",\n    \"immigration\", \"industry_news\", \"technology\",\n    \"events\", \"legal_precedents\",\n]\n\nclass RawArticle(BaseModel):\n    id: str\n    title: str\n    url: HttpUrl\n    source: str\n    source_category: Literal[\"legal\", \"news\", \"government\", \"industry\"]\n    content: str\n    published_date: Optional[datetime]\n\nclass ProcessedArticle(BaseModel):\n    id: str\n    title: str\n    url: HttpUrl\n    source: str\n    source_category: str\n    content: str\n    summary: Optional[str]\n    region: RegionCode\n    country: str\n    topics: List[TopicCode]\n    relevance_score: float\n    reasoning: str\n    key_signals: List[str]\n    published_date: Optional[datetime]\n    ingested_at: datetime\n```\n\nProcessing log schema:\n```python\nclass ProcessingLogEntry(BaseModel):\n    article_id: str\n    stage: Literal[\n        \"ingested\", \"deduplicated\", \"filtered_relevant\",\n        \"filtered_irrelevant\", \"classified\", \"scored\",\n        \"summarized\", \"stored_weaviate\", \"error\",\n    ]\n    timestamp: datetime\n    details: dict\n```\n- Implement a simple `logging/audit.py` helper that appends JSON lines to `processing_log.jsonl` using orjson or standard `json`:\n```python\nfrom pathlib import Path\nfrom datetime import datetime\nimport json\nfrom .models import ProcessingLogEntry\n\nLOG_PATH = Path(\"logs/processing_log.jsonl\")\nLOG_PATH.parent.mkdir(exist_ok=True)\n\ndef log_stage(entry: ProcessingLogEntry) -> None:\n    with LOG_PATH.open(\"a\", encoding=\"utf-8\") as f:\n        f.write(entry.model_dump_json() + \"\\n\")\n```\n\nBest practices:\n- Use `RegionCode` and `TopicCode` enums throughout (including DSPy classifiers) to avoid drift from taxonomy.\n- Centralize any mapping of external source identifiers to `source_category` values.\n- Ensure `id` strategy is consistent (e.g., stable hash of `url+published_date` or UUID).\n",
        "testStrategy": "- Add unit tests for model validation:\n  - Creating `RawArticle` and `ProcessedArticle` with valid data should pass.\n  - Invalid region/topic codes should raise validation errors.\n  - Missing required fields (e.g., title) should fail.\n- Test `ProcessingLogEntry` serialization and round-trip:\n  - Write a sample entry to `processing_log.jsonl` and read it back, ensuring JSON lines are valid and parseable.\n- Use `pytest` with `tmp_path` to verify that audit logging appends without truncating existing data.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "RSS Ingestion and Raw Article Queue",
        "description": "Implement the daily RSS ingestion job for 20+ tier-1 feeds, parsing entries into RawArticle objects and storing them in a raw queue for downstream processing.",
        "details": "Implementation details:\n- Create `ingestion/rss_ingestor.py` using `feedparser`.\n- Maintain a YAML/JSON config `config/rss_feeds.json` listing the initial ~20 tier-1 RSS URLs, each with fields: `name`, `url`, `source_category`.\n- For each feed, fetch once per daily batch:\n  - Use `requests` or `httpx` with sensible timeouts and retries; respect RSS caching headers.\n  - Parse via `feedparser.parse(response.content)`.\n- Normalize entries into `RawArticle`:\n  - `id`: deterministic (e.g., `hashlib.sha256((link+title).encode()).hexdigest()`).\n  - `content`: prefer `content[0].value` else `summary`.\n  - `published_date`: parse from `published_parsed` if available.\n- Save raw articles into a filesystem-based `raw_queue/` as JSONL or one JSON per file:\n  - Simple approach: one JSONL per day: `raw_queue/2026-01-12.jsonl`.\n- Ensure idempotency:\n  - Keep a small local SQLite or JSON store of seen IDs (or last published date per feed) to avoid repeated ingestion within same day.\n- Wire in audit logging:\n  - For each successfully parsed article, log a `ProcessingLogEntry(stage=\"ingested\", ...)`.\n\nPseudocode:\n```python\nimport feedparser, hashlib, json\nfrom datetime import datetime\nfrom pathlib import Path\nfrom models import RawArticle\nfrom logging.audit import log_stage, ProcessingLogEntry\n\nRAW_DIR = Path(\"raw_queue\")\nRAW_DIR.mkdir(exist_ok=True)\n\ndef fetch_feeds(feeds_cfg):\n    today_file = RAW_DIR / f\"{datetime.utcnow().date()}.jsonl\"\n    with today_file.open(\"a\", encoding=\"utf-8\") as f:\n        for feed in feeds_cfg:\n            parsed = feedparser.parse(feed[\"url\"])\n            for entry in parsed.entries:\n                url = entry.get(\"link\")\n                if not url:\n                    continue\n                aid = hashlib.sha256((url + entry.get(\"title\", \"\")).encode()).hexdigest()\n                art = RawArticle(\n                    id=aid,\n                    title=entry.get(\"title\", \"\"),\n                    url=url,\n                    source=feed[\"name\"],\n                    source_category=feed[\"source_category\"],\n                    content=_extract_content(entry),\n                    published_date=_parse_date(entry),\n                )\n                f.write(art.model_dump_json() + \"\\n\")\n                log_stage(ProcessingLogEntry(\n                    article_id=aid,\n                    stage=\"ingested\",\n                    timestamp=datetime.utcnow(),\n                    details={\"source\": feed[\"name\"]},\n                ))\n```\n\nScheduling:\n- Use system `cron` to run a top-level `python -m ingestion.run_batch` daily at `06:00 UTC`.\n- `run_batch` orchestrates: RSS ingestion → save raw queue; later tasks will attach dedup and processing.\n",
        "testStrategy": "- Unit tests:\n  - Mock `feedparser.parse` to return a predictable feed and assert correct `RawArticle` objects are produced.\n  - Verify invalid/missing URLs are skipped gracefully.\n- Integration test:\n  - Run `fetch_feeds` against 1-2 real RSS feeds in a test environment and confirm:\n    - A new JSONL file is created in `raw_queue/`.\n    - Each line parses to a valid `RawArticle`.\n- Idempotency test:\n  - Run ingestion twice with the same mocked feed and confirm duplicates are either prevented or clearly marked for later dedup (depending on final strategy).\n- Ensure the job exits with non-zero on fatal errors and logs exceptions for monitoring.",
        "priority": "high",
        "dependencies": [
          1,
          2
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Deduplication Module (Hash + Fuzzy Matching)",
        "description": "Implement deduplication over the raw article queue using exact hash and fuzzy content similarity to reduce to ~400 unique daily articles.",
        "details": "Implementation details:\n- Create `pipeline/deduplicate.py` that reads the day’s raw JSONL from `raw_queue/` and outputs a deduplicated JSONL to `work_queue/deduped/`.\n- Exact duplicate removal:\n  - Use article `id` from ingestion as primary key.\n  - Maintain an in-memory `seen_ids` set per run; for long-term, optionally persist a small SQLite table of processed IDs to avoid cross-day duplicates.\n- Fuzzy duplicate detection:\n  - Compute a `content_hash` or fingerprint based on normalized text:\n    - Lowercase, strip HTML, collapse whitespace.\n  - Use a fast similarity library (e.g., `rapidfuzz`) or a simple cosine similarity over TF-IDF with `scikit-learn` for current scale; target threshold >0.9 similarity for dropping near-duplicates.\n  - For minimal dependencies, a direct `difflib.SequenceMatcher` with a cutoff of 0.9 can work, but for ~500 items daily this is still cheap.\n- Algorithm outline:\n  - Iterate articles, keep a list of canonical contents.\n  - For each new article, compare against a small window of previous contents (e.g., last 200) with similarity measure.\n  - If similarity >0.9 with any, mark as duplicate and log `stage=\"deduplicated\"` with reason.\n  - Else, keep article and write to `deduped/YYYY-MM-DD.jsonl`.\n- Ensure deterministic behavior so that reruns with the same input yield identical outputs.\n\nPseudocode:\n```python\nfrom rapidfuzz import fuzz\nfrom models import RawArticle\n\nSIM_THRESHOLD = 90  # 0-100\n\ncanonical = []  # list of (id, text)\n\nfor art in raw_articles:\n    text = normalize(art.content)\n    is_dup = False\n    for _, ctext in canonical[-200:]:\n        if fuzz.token_set_ratio(text, ctext) >= SIM_THRESHOLD:\n            is_dup = True\n            break\n    if is_dup:\n        log_stage(... stage=\"deduplicated\", details={\"reason\": \"fuzzy\"})\n        continue\n    canonical.append((art.id, text))\n    write_to_deduped(art)\n```\n\nBest practices:\n- Cap memory usage by not retaining full contents for previous days within a single run.\n- Log both number of raw vs deduplicated articles for monitoring.\n",
        "testStrategy": "- Unit tests with synthetic data:\n  - Two identical articles should result in only one output after dedup.\n  - Slightly reworded but semantically identical articles should be flagged as duplicates based on similarity threshold.\n  - Clearly distinct articles must not be removed.\n- Regression test:\n  - Run dedup twice on the same input file and confirm the deduplicated output is identical.\n- Performance test:\n  - With ~1,000 synthetic articles, ensure dedup completes well under a few seconds on a typical dev machine.",
        "priority": "high",
        "dependencies": [
          2,
          3
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "DSPy Tiny LM Relevance Pre-Filter (RelevancePreFilter Signature)",
        "description": "Implement the DSPy-based tiny LM semantic relevance filter using gpt-4o-mini to reduce ~400 deduped articles to ~150–200 candidates per day.",
        "details": "Implementation details:\n- Define the `RelevancePreFilter` `dspy.Signature` exactly as specified in the PRD in `dspy_modules/relevance_filter.py`.\n- Configure DSPy to use `openai/gpt-4o-mini` with low temperature (e.g., 0.2) for consistent decisions.\n- Wrap the signature with `dspy.Predict` for fast, low-cost calls:\n```python\nimport dspy\nfrom .signatures import RelevancePreFilter\n\nclass RelevancePreFilterModule(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.predict = dspy.Predict(RelevancePreFilter)\n\n    def forward(self, title, content_preview, source_category):\n        return self.predict(\n            title=title,\n            content_preview=content_preview[:500],\n            source_category=source_category,\n        )\n```\n- Integrate into pipeline `pipeline/filter_relevance.py`:\n  - Read `deduped/YYYY-MM-DD.jsonl` → `RawArticle` instances.\n  - For each, call the module and inspect `is_relevant` and `confidence`.\n  - Persist only relevant ones (or those above a configurable confidence threshold) to `work_queue/relevant/YYYY-MM-DD.jsonl`.\n  - Log `stage=\"filtered_relevant\"` or `\"filtered_irrelevant\"` in audit log.\n- Batch processing:\n  - For cost and latency, implement simple micro-batching if needed (but OpenAI Python client already optimizes HTTP under the hood; keep it simple initially).\n- Add configuration in a central `config/settings.py`:\n  - `MIN_RELEVANCE_CONFIDENCE = 0.4` (tunable later).\n\nBest practices:\n- Include minimal context in `content_preview` to stay under token goals.\n- Ensure the module can be later compiled/optimized with BootstrapFewShot (Task 9) without changing interfaces.\n",
        "testStrategy": "- Unit tests:\n  - Monkeypatch DSPy `LM` to return deterministic responses for small test inputs, verifying `is_relevant` handling and thresholding work.\n- Pipeline test:\n  - Pass a small deduped file containing a mix of clearly relevant (e.g., explicit FCRA/GDPR) and unrelated content (e.g., sports news) and assert that most unrelated items are filtered out.\n- Cost/latency check:\n  - Time processing of 100 test articles and extrapolate to 500/day to ensure within <5 minutes for this stage.\n- Add assertions to ensure the number of candidates per run is logged, to be monitored against the 150–200 target over time.",
        "priority": "high",
        "dependencies": [
          1,
          2,
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "DSPy Classification, Scoring, and Summarization Pipeline",
        "description": "Implement the DSPy-based classification (region, topics, country), relevance scoring with reasoning, and 2–3 sentence summarization stages over relevant articles.",
        "details": "Implementation details:\n- Implement the three signatures exactly as defined in the PRD in `dspy_modules/signatures.py`:\n  - `ArticleClassifier` using `dspy.TypedPredictor`.\n  - `RelevanceScorer` using `dspy.ChainOfThought`.\n  - A summarization signature (e.g., `ArticleSummarizer`) matching the description (2–3 sentence industry-focused summary) with `dspy.Predict`.\n\nExample modules:\n```python\nimport dspy\nfrom .signatures import ArticleClassifier, RelevanceScorer, ArticleSummarizer\n\nclass ArticleClassifierModule(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.classifier = dspy.TypedPredictor(ArticleClassifier)\n\n    def forward(self, title, content, source):\n        return self.classifier(title=title, content=content[:2000], source=source)\n\nclass RelevanceScorerModule(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.scorer = dspy.ChainOfThought(RelevanceScorer)\n\n    def forward(self, title, content, region, topics):\n        return self.scorer(\n            title=title,\n            content=content[:1500],\n            region=region,\n            topics=topics,\n        )\n\nclass SummarizerModule(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.summarize = dspy.Predict(ArticleSummarizer)\n\n    def forward(self, title, content):\n        return self.summarize(title=title, content=content[:2000])\n```\n\n- Create `pipeline/process_articles.py` that:\n  - Reads relevant articles from `work_queue/relevant/YYYY-MM-DD.jsonl`.\n  - For each article:\n    1. Calls `ArticleClassifierModule` → `region`, `country`, `topics`, `confidence`.\n    2. Calls `RelevanceScorerModule` → `relevance_score`, `key_signals`, `reasoning`.\n    3. If `relevance_score >= SCORE_THRESHOLD` (configurable, e.g., 0.6), calls `SummarizerModule` and attaches the summary.\n  - Converts to `ProcessedArticle` and writes to `work_queue/processed/YYYY-MM-DD.jsonl`.\n  - Logs `classified`, `scored`, and `summarized` stages per article.\n\nBest practices:\n- Use the taxonomy enums from Task 2 to validate model outputs; if invalid region/topic is produced, map to `worldwide` or drop topic with a warning.\n- Keep temperature low to moderate; for Chain-of-Thought scoring, a slightly higher temperature (0.3–0.4) can help diverse reasoning.\n- Catch and log any model/API failures but continue processing other articles.\n",
        "testStrategy": "- Unit tests:\n  - Mock DSPy modules to return deterministic outputs and ensure the pipeline correctly chains classification → scoring → summarization and writes a `ProcessedArticle` JSONL.\n- Validation tests:\n  - Ensure that invalid regions/topics from the model are either rejected or safely normalized and logged.\n- Integration test with live LMs (in staging):\n  - Run the pipeline on a small batch of 10–20 real articles, then manually inspect region/topic labels, scores, and summaries for plausibility.\n- Performance test:\n  - Measure processing time for ~200 candidates and ensure total DSPy stages fit within the overall <2-hour daily batch budget when combined with other tasks.",
        "priority": "high",
        "dependencies": [
          1,
          2,
          5
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Weaviate Storage Integration for Processed Articles",
        "description": "Store processed articles into the Weaviate `NewsletterArticles` collection with embeddings and metadata, and maintain the JSONL processing log.",
        "details": "Implementation details:\n- Create `storage/weaviate_client.py` to centralize connection logic:\n```python\nimport weaviate\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\ndef get_client():\n    url = os.getenv(\"WEAVIATE_URL\", \"http://localhost:8080\")\n    return weaviate.connect_to_local(host=url.replace(\"http://\", \"\").split(\":\")[0])\n```\n- Implement `storage/store_articles.py` that:\n  - Reads `work_queue/processed/YYYY-MM-DD.jsonl` into `ProcessedArticle` instances.\n  - Connects to Weaviate and gets the `NewsletterArticles` collection.\n  - Uses the Weaviate Python client’s batch API to insert objects:\n    - `title`, `content`, `summary`, `url`, `source`, `region`, `country`, `topics`, `relevance_score`, `reasoning`, `published_date`, `ingested_at`.\n  - Let Weaviate’s `text2vec-openai` module generate embeddings using `text-embedding-3-small` (already configured in the schema) based on `content`.\n- Ensure `OPENAI_API_KEY` and `COHERE_API_KEY` env vars are available to the Weaviate container as per docker-compose.\n- For each successful insert, log `stage=\"stored_weaviate\"` with the article ID and Weaviate UUID.\n- Also write a daily backup JSONL (e.g., `archive/YYYY-MM-DD.jsonl`) for offline analysis.\n\nPseudocode:\n```python\nfrom models import ProcessedArticle\nfrom logging.audit import log_stage, ProcessingLogEntry\nfrom datetime import datetime\n\nclient = get_client()\ncollection = client.collections.get(\"NewsletterArticles\")\n\nwith batch_context(collection) as batch:\n    for art in processed_articles:\n        obj = {\n            \"title\": art.title,\n            \"content\": art.content,\n            \"summary\": art.summary or \"\",\n            \"url\": str(art.url),\n            \"source\": art.source,\n            \"region\": art.region,\n            \"country\": art.country,\n            \"topics\": art.topics,\n            \"relevance_score\": art.relevance_score,\n            \"reasoning\": art.reasoning,\n            \"published_date\": art.published_date,\n            \"ingested_at\": art.ingested_at,\n        }\n        uuid = collection.data.insert(obj)\n        log_stage(ProcessingLogEntry(\n            article_id=art.id,\n            stage=\"stored_weaviate\",\n            timestamp=datetime.utcnow(),\n            details={\"uuid\": str(uuid)},\n        ))\n```\n\nBest practices:\n- Use batch size ~50–100 to balance throughput and memory.\n- Handle transient Weaviate or network errors with retries and clear logging.\n- Avoid embedding very long content; ensure `content` was already truncated reasonably in earlier stages.\n",
        "testStrategy": "- Unit tests with a mocked Weaviate client:\n  - Verify that objects are created with correct property names and data types.\n- Integration test:\n  - Insert 2–3 sample `ProcessedArticle` objects into a real local Weaviate and query them back using hybrid search to confirm fields are persisted.\n- Failure tests:\n  - Simulate Weaviate downtime (e.g., by pointing to wrong URL) and ensure the script fails gracefully and logs `stage=\"error\"` entries without data corruption.\n- Confirm that `processing_log.jsonl` contains `stored_weaviate` entries for all successfully stored articles.",
        "priority": "high",
        "dependencies": [
          1,
          2,
          6
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "QUIPLER-based Retrieval and ReAct Query Agent Implementation",
        "description": "Implement the QUIPLER-based retrieval layer over Weaviate and the DSPy ReAct query agent with the specified tools for filtering and article detail access.",
        "details": "Implementation details:\n- Install and import `retrieve-dspy` and its `QUIPLER` plus `RerankerClient` classes as per PRD.\n- Implement `query/agent.py` containing `NewsletterQueryAgent` closely following the PRD example.\n- Initialize Weaviate and Cohere clients only once per process for efficiency.\n- QUIPLER configuration:\n  - `collection_name=\"NewsletterArticles\"`.\n  - `target_property_name=\"content\"`.\n  - `retrieved_k=50`, `reranked_k=20`, `rrf_k=60` as specified.\n  - Use Cohere `rerank-v3.5` via `RerankerClient(name=\"cohere\", client=cohere_client)`.\n- Implement ReAct tools that operate directly on Weaviate:\n```python\nclass NewsletterQueryAgent(dspy.Module):\n    ...\n    def filter_by_date(self, start_date: str, end_date: str) -> str:\n        # Query Weaviate with a where filter on published_date\n        # Return serialized list or summary of article IDs/titles\n\n    def filter_by_region(self, region: str) -> str:\n        # Where filter on region property\n\n    def filter_by_topic(self, topic: str) -> str:\n        # Where filter on topics array, e.g., contains topic\n\n    def get_article_details(self, article_id: str) -> str:\n        # Query by Weaviate UUID or by custom id property if stored\n        # Return title, date, summary, url as a text blob\n```\n- Implement `_format_context` exactly as in PRD, limiting to ~15 top sources and including `[index]` labels for citations.\n- `forward(question, filters=None)` pipeline:\n  - Calls `self.retriever.forward(...)` with QUIPLER and Cohere reranker.\n  - Applies any explicit filters from CLI (region, date, topic) on the retrieved set if desired.\n  - Calls `self.synthesize` (ReAct) with `QueryResponseSignature` and tools list.\n  - Returns a `dspy.Prediction` object with `answer`, `confidence`, `cited_articles`, `follow_up_suggestions`, `queries_generated`, `total_documents_searched`.\n\nBest practices:\n- Keep QUIPLER `verbose=False` in production to avoid noisy logs; add a debug flag in config if detailed traces are needed.\n- Ensure tools are side-effect free and only read from Weaviate.\n- Carefully craft the `QueryResponseSignature` description to encourage inline citation markers `[1]`, `[2]` corresponding to `_format_context` indices.\n",
        "testStrategy": "- Unit tests with mocked Weaviate and Cohere clients:\n  - QUIPLER retrieval returns synthetic `sources`; verify `_format_context` produces expected string and indices.\n  - ReAct tools return predictable strings; assert they are callable from the agent without exceptions.\n- Integration test with a small Weaviate dataset (5–10 articles):\n  - Ask a simple query and verify the agent returns an answer string, a non-empty list of `cited_articles`, and that article IDs match existing Weaviate objects.\n- Latency test:\n  - Measure end-to-end response time for a few representative queries, ensuring simple queries typically stay <5–10 seconds under local conditions.\n- Edge-case tests:\n  - No matching articles: agent should respond gracefully, with low confidence and empty citations.\n  - Invalid filters: ensure they are either ignored or return a clear message, not crashes.",
        "priority": "high",
        "dependencies": [
          1,
          2,
          7
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "CLI Interface (Click + Rich) and Batch Orchestration",
        "description": "Provide a user-friendly CLI for querying, listing candidates, and viewing statistics, and wire cron-based orchestration for the daily batch pipeline.",
        "details": "Implementation details:\n- Implement `cli.py` using `click` and `rich` for formatting.\n- Commands as per PRD examples:\n  - `query`:\n    - Usage: `python cli.py query \"question\" [--region REGION] [--days N]`.\n    - Instantiates `NewsletterQueryAgent`, computes optional date range filter for `--days`, passes filters dict to `forward`.\n    - Pretty-prints answer, confidence, and cited articles with Rich tables.\n  - `list`:\n    - Usage: `python cli.py list --date YYYY-MM-DD --min-score FLOAT`.\n    - Queries Weaviate for articles with `ingested_at` on given date and `relevance_score >= min-score`.\n    - Outputs title, score, region, topics, URL in a table.\n  - `stats`:\n    - Usage: `python cli.py stats --days N`.\n    - Aggregates counts from Weaviate and/or `processing_log.jsonl` (e.g., ingested, filtered, stored, candidates per day) over last N days.\n- Implement a top-level batch runner `pipeline/run_daily_batch.py`:\n  - Steps at high level:\n    1. RSS ingestion (Task 3).\n    2. Deduplication (Task 4).\n    3. Relevance filtering (Task 5).\n    4. Classification / scoring / summarization (Task 6).\n    5. Weaviate storage (Task 7).\n  - Each step logs start/end and error counts.\n- Configure system `cron` (outside code) to run:\n  - `0 6 * * * /usr/bin/python /path/to/pipeline/run_daily_batch.py >> logs/batch.log 2>&1`.\n\nPseudocode (Click command skeleton):\n```python\nimport click\nfrom query.agent import NewsletterQueryAgent\n\n@click.group()\ndef cli():\n    pass\n\n@cli.command()\n@click.argument(\"question\")\n@click.option(\"--region\", type=str)\n@click.option(\"--days\", type=int)\ndef query(question, region, days):\n    filters = {}\n    if region:\n        filters[\"region\"] = region\n    if days:\n        filters[\"days\"] = days\n    agent = NewsletterQueryAgent(...)\n    result = agent.forward(question, filters=filters)\n    # print with Rich\n\nif __name__ == \"__main__\":\n    cli()\n```\n",
        "testStrategy": "- CLI tests using `click.testing.CliRunner`:\n  - Invoke `query` with a mocked `NewsletterQueryAgent` and verify output formatting and exit code 0.\n  - Test `list` and `stats` with mocked Weaviate and/or sample log files.\n- End-to-end batch test in staging:\n  - Run `run_daily_batch.py` on a controlled subset of feeds and inspect that each intermediate file and Weaviate insertion step completed.\n  - Verify `processing_log.jsonl` contains entries for all stages.\n- Confirm `cron` integration by running the scheduled command manually and checking logs for correct operation.",
        "priority": "medium",
        "dependencies": [
          3,
          4,
          5,
          6,
          7,
          8
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Data Labeling, DSPy Optimization (BootstrapFewShot), and Monitoring",
        "description": "Collect labeled examples for key DSPy signatures, run BootstrapFewShot optimization to reach ≥70% classification accuracy/recall, and implement basic monitoring dashboards for quality and cost.",
        "details": "Implementation details:\n- Data labeling:\n  - Build a simple offline script `tools/export_for_labeling.py` to export samples from `processing_log` + `Weaviate` into CSV/JSON for annotation (fields: title, content snippet, current labels, model predictions).\n  - Target labeled counts from PRD: ~50 for RelevancePreFilter, 100–150 for ArticleClassifier, 50–100 for RelevanceScorer, 20–30 for QueryResponse.\n  - Store gold labels in `data/labels/*.jsonl`.\n- DSPy optimization:\n  - Follow current DSPy best practices from docs and community guides[2][3]:\n    - Use `dspy.BootstrapFewShot` as optimizer over each signature with an appropriate metric (e.g., accuracy or F1 for classification, correlation-like heuristic for scores).\n  - Example for classifier:\n```python\nimport dspy\nfrom dspy.evaluate import Evaluate\nfrom dspy.optim import BootstrapFewShot\n\ntrain_data = ...  # labeled examples list\nmetric = ...      # custom metric comparing predicted vs gold region/topics\n\noptimizer = BootstrapFewShot(metric=metric, max_bootstrapped_demos=30)\ncompiled_classifier = optimizer.compile(ArticleClassifierModule(), train_data)\n# Save compiled program\ncompiled_classifier.save(\"art_classifier_compiled.json\")\n```\n  - Load compiled modules in production (Tasks 5–8) instead of zero-shot ones once evaluation shows improvement.\n- Monitoring:\n  - Implement a lightweight metrics script `monitoring/metrics.py` that:\n    - Reads `processing_log.jsonl` and Weaviate counts.\n    - Computes daily: number ingested, filtered, candidate count, mean relevance_score, approximate recall vs manually curated newsletter (once available).\n  - Optionally integrate with `mlflow` (as recommended in DSPy tutorials[1][6]) for tracking optimization experiments.\n\nBest practices:\n- Keep train/validation split to avoid overfitting; aim for at least 20–30% hold-out.\n- Version labeled datasets and compiled programs so you can rollback if accuracy drops.\n- Monitor monthly API cost by counting calls/tokens from logs and comparing with the <$50/month target.\n",
        "testStrategy": "- Unit tests for custom metrics used in DSPy optimization to ensure they correctly reward accurate region/topic predictions.\n- Dry-run optimization with a tiny labeled set in CI to ensure the code path is valid (without depending on large LM budgets).\n- After deploying compiled modules in staging, run an A/B evaluation:\n  - Compare baseline vs optimized classifier on the same labeled validation set and assert ≥15–25% improvement over zero-shot, moving towards ≥70% accuracy/recall.\n- Validate monitoring output:\n  - Run `monitoring/metrics.py` on sample logs and inspect that metrics match manual calculations.\n- Periodically (e.g., weekly) have a human review a random sample of classified and scored articles to verify the system maintains quality; track issues for future retraining.",
        "priority": "medium",
        "dependencies": [
          5,
          6,
          8
        ],
        "status": "pending",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2026-01-12T05:54:25.119Z",
      "updated": "2026-01-12T05:54:25.119Z",
      "description": "Tasks for master context"
    }
  }
}